<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Welcome to qxdn Blog</title>
    <url>/2020/10/17/helloworld/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>保研后无所事事，同时想起来自己的GitHub Page一直没有用过，因此借用了<a href="https://github.com/Huxpro">Huxpro</a>的<a href="https://github.com/Huxpro/huxpro.github.io">Hux Blog</a>来搭建自己的GitHub page</p>
<h2 id="正文"><a class="markdownIt-Anchor" href="#正文"></a> 正文</h2>
<p>Hello World</p>
<p>这里是<a href="https://github.com/qxdn">qxdn</a>的Blog，也是第一篇Blog</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title>电赛</title>
    <url>/2020/10/19/electronicDesignContest/</url>
    <content><![CDATA[<h2 id="电赛选题"><a class="markdownIt-Anchor" href="#电赛选题"></a> 电赛选题</h2>
<p>我作为组长和<a href="https://github.com/while0l1">李志豪</a>、<a href="https://github.com/renruiwhut">任睿</a>一起参加了2019年的电赛。说到组长的选择其实也是很随意，就是投骰子比小，最小的做组长。2019年的题目可以见<a href="https://www.nuedc-training.com.cn/index/news/details/new_id/154.html">这里</a>，我们当时主要在F题和H题里面纠结，我主张F题，而李志豪强烈反对，因为我们缺少传感器，而且一时没有解决方法，因此我们选择了H题。</p>
<span id="more"></span>
<h2 id="题目要求"><a class="markdownIt-Anchor" href="#题目要求"></a> 题目要求</h2>
<p>题目的具体要求和测试结果见<a href="https://www.nuedc-training.com.cn/index/news/details/new_id/153">这</a></p>
<p><img src="/images/electronicDesignContest/problem.png" alt="参考图片" /></p>
<h3 id="基本要求"><a class="markdownIt-Anchor" href="#基本要求"></a> 基本要求</h3>
<ol>
<li>电磁炮能够将弹丸射出炮口。</li>
<li>环形靶放置在靶心距离定标点200~300cm间，且在中心轴线上的位置处，键盘输入距离d值，电磁炮将弹丸发射至该位置，距离偏差的绝对值不大于50cm。</li>
<li>用键盘给电磁炮输入环形靶中心与定标点的距离d 及与中心轴线的偏离角度a，一键启动后，电磁炮自动瞄准射击，按击中环形靶环数计分；若脱靶则不计分。</li>
</ol>
<h3 id="发挥部分"><a class="markdownIt-Anchor" href="#发挥部分"></a> 发挥部分</h3>
<ol>
<li>在指定范围内任意位置放置环形靶（有引导标识，参见说明2），一键启动后，电磁炮自动搜寻目标并炮击环形靶，按击中环形靶环数计分，完成时间≤30s。</li>
<li>环形靶与引导标识一同放置在距离定标点d=250cm 的弧线上（以靶心定位），引导标识处于最远位置。电磁炮放置在定标点，炮管水平方向与中轴线夹角a =-30°、仰角0°。一键启动电磁炮，炮管在水平方向与中轴线夹角a从-30°至30°、再返回-30°做往复转动，在转动过程中（中途不得停顿）电磁炮自动搜寻目标并炮击环形靶，按击中环形靶环数计分，启动至击发完成时间≤10s。</li>
<li>其他。</li>
</ol>
<h2 id="制作电磁炮"><a class="markdownIt-Anchor" href="#制作电磁炮"></a> 制作电磁炮</h2>
<p>使用漆包线包裹发射管，我们制作了电磁炮的炮管。使用Boost电路制作升压，电容来存储能量。关于控制电磁炮发射距离使用的是固定发射电压、调整发射角度的方法，使用的方法是使用MATLAB进行数据拟合。测量距离使用openmv中距离和像素的大小成反比的关系。系统结构图、DC-DC电路如下图所示。<br />
<img src="/images/electronicDesignContest/structure.png" alt="系统结构图" /><br />
<img src="/images/electronicDesignContest/DC-DC.png" alt="DC-DC" /></p>
<h2 id="实物"><a class="markdownIt-Anchor" href="#实物"></a> 实物</h2>
<p><img src="/images/electronicDesignContest/railgun.jpg" alt="railgun" /></p>
<h2 id="报告"><a class="markdownIt-Anchor" href="#报告"></a> 报告</h2>
<p><a href="/others/railgunReport.docx">报告</a></p>
<h2 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h2>
<p>我们组的电赛代码在<a href="https://github.com/qxdn/BIg-Ivan">这</a>，取名大伊万（笑）。这里面还包含一些训练时期的项目，都在git分支里面。</p>
<h2 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h2>
<p>在省赛和国赛的时候都打了全十环（不得不说运气真好），然后公费去上海同济玩了几天。最终结果就是获得了一个H题的国一了，不算国家奖学金的话，这是我获得的第一个竞赛国奖。<br />
<img src="/images/electronicDesignContest/edc.jpg" alt="同济现场" /></p>
]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>电赛</tag>
        <tag>C</tag>
        <tag>C++</tag>
        <tag>软件</tag>
        <tag>硬件</tag>
      </tags>
  </entry>
  <entry>
    <title>电赛准备</title>
    <url>/2020/10/19/electronicDesignContestPerpare/</url>
    <content><![CDATA[<h2 id="进入606实验室"><a class="markdownIt-Anchor" href="#进入606实验室"></a> 进入606实验室</h2>
<p>  加入实验室需要经过笔试以及面试。招生时间通常为大一下到大二上这段时间。笔试期间主要为《电路分析》、《模拟电子技术基础》和《数字电子技术基础》里面的知识，还包括单片机、C语音以及IDE的一些知识。面试主要看一些个人项目。我主要展示了一些在403实验室制作的水温控制系统、万年历还有个人制作的简易示波器（因为当时对ADC不是很熟练，而且不知道奈奎斯特抽样定理，所以做的很尴尬）。进入实验室后和学长进行双选，因为当时和张俊伟、李洋一起被分配做电源，所以找了余世民学长做电源。</p>
<h2 id="606实验室学习"><a class="markdownIt-Anchor" href="#606实验室学习"></a> 606实验室学习</h2>
<p>  在实验室的上半个学期主要为学习，以及完成实验室的任务。实验室有着淘汰机制，没有完成阶段任务，在最后就会被淘汰。阶段任务主要为三极管放大电路、运放电路、滤波器等。虽然题目看起来简单，但是实际做起来还是与书上的理想情况不一样。幸好最后没有被淘汰。</p>
<h2 id="转为控制组"><a class="markdownIt-Anchor" href="#转为控制组"></a> 转为控制组</h2>
<p>  在一段时间的电源制作（其实没有）后，我反而对控制题目比较感兴趣，就加入了<a href="https://github.com/while0l1">李志豪</a>、<a href="https://github.com/renruiwhut">任睿</a>的小组，一起做控制题。制作了一阶倒立摆、自由摆、板球控制系统、麦克纳姆轮四轮车、悬挂系统、风力摆、二轮循迹平衡车、XY写字机器人等项目。</p>
<h2 id="实物图片"><a class="markdownIt-Anchor" href="#实物图片"></a> 实物图片</h2>
<p><img src="/images/electronicDesignContestPrepare/flb.gif" alt="风力摆" /><br />
<img src="/images/electronicDesignContestPrepare/balanceCar.gif" alt="二轮平衡车" /><br />
<img src="/images/electronicDesignContestPrepare/XYWriter.gif" alt="XY写字机器人" /></p>
<h2 id="后记"><a class="markdownIt-Anchor" href="#后记"></a> 后记</h2>
<p>  这些就是我们组在电赛期间的一些准备内容，代码主要在<a href="https://github.com/while0l1/stm32f4_modules">代码1</a>和<a href="https://github.com/qxdn/MYDS">代码2</a>和<a href="https://github.com/qxdn/BIg-Ivan">训练代码</a></p>
]]></content>
      <categories>
        <category>C</category>
      </categories>
      <tags>
        <tag>电赛</tag>
        <tag>C</tag>
        <tag>C++</tag>
        <tag>软件</tag>
        <tag>硬件</tag>
      </tags>
  </entry>
  <entry>
    <title>节能减排</title>
    <url>/2020/10/19/energySaving/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>原先的校内初选中，我最先参加的与能动学院一起的节能减排做纵倾优化的船，但是没有通过初审。于是和汪博文、赵之清、胡群强、王瑞林、任睿、孙文滨一起参加节能减排大赛，制作光伏屋顶。</p>
<h2 id="研制背景"><a class="markdownIt-Anchor" href="#研制背景"></a> 研制背景</h2>
<p>传统光伏屋顶的发电效率比较低，因此我们考虑将一部分光伏板换成天窗使用。由于天窗会有灰尘雨水等，比较容易脏，需要人力清除成本。因此我们对天窗挡板做智能处理，能够根据天气光照自动控制开合角度，使得光照最大化同时减少人力清理成本。根据贾博文老师说，这个貌似是根据他在新加坡看到的直接更改的。</p>
<h2 id="实物"><a class="markdownIt-Anchor" href="#实物"></a> 实物</h2>
<p>我们主要制作了<a href="http://101.133.235.188/">智能设计平台</a>(现在应该上不去了)、手机控制平台和光伏屋顶。三个之间的通信主要是使用MQTT协议。<br />
<img src="/images/energySavingContest/design.png" alt="智能设计网页" /><br />
<img src="/images/energySavingContest/controller.png" alt="智能控制平台" /><br />
<img src="/images/energySavingContest/roof.gif" alt="光伏屋顶" /></p>
<h2 id="代码"><a class="markdownIt-Anchor" href="#代码"></a> 代码</h2>
<p><a href="https://github.com/qxdn/sunbest">智能设计平台</a>、<a href="https://github.com/qxdn/WindowController">天窗控制平台</a>和<a href="https://github.com/qxdn/EnergySaveRoof">实物与8266代码</a>因为疫情原因而且组员也不是很会因此实物代码比较老、但是8266的代码是最新的。</p>
<h2 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h2>
<p>最后还是只得了一个国三，如果再做的好一点也许就能过了网申冲击国二、国一了。</p>
]]></content>
      <categories>
        <category>web</category>
      </categories>
      <tags>
        <tag>C</tag>
        <tag>C++</tag>
        <tag>软件</tag>
        <tag>硬件</tag>
        <tag>节能减排</tag>
        <tag>java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title>用AlexNet实现DogsVsCats</title>
    <url>/2020/10/22/DogsVsCatsAlexNet/</url>
    <content><![CDATA[<h2 id="alexnet-实现猫狗大战"><a class="markdownIt-Anchor" href="#alexnet-实现猫狗大战"></a> AlexNet 实现猫狗大战</h2>
<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>早就听说过tensorflow的大名，但一直没有时间仔细研究，而且一直缺少数据集，也就没怎么了解。同时因为那tensorflow1的复杂静态图一直劝退。如今有了时间，而且tensorflow2更加易懂，因此仔细研究了一下，也就是第一次炼丹。首先是在<a href="https://www.kaggle.com/">kaggle</a>下载数据集就难到我了，没想到翻了墙还被困在手机号验证。最后只能借助于微软提供的数据集下载。最开始用自己的CPU跑又慢又卡，自从我换了硬盘加了内存以来，好久都没有这么卡过了。</p>
<p><img src="/images/DogsVsCats/overload.png" alt="overload" /></p>
<p>随后换上了Google的<a href="https://colab.research.google.com/notebooks/intro.ipynb#">colab</a>,用了免费GPU才知道GPU的好。</p>
<blockquote>
<p>以下正文</p>
</blockquote>
<h2 id="1-数据准备"><a class="markdownIt-Anchor" href="#1-数据准备"></a> 1、数据准备</h2>
<p>在Colab上需要下载数据和准备环境，已经将数据上传到了Github</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install tensorflow</span><br><span class="line">!wget https://github.com/qxdn/DogsVsCats/releases/download/v1<span class="number">.0</span><span class="number">.0</span>/data.<span class="built_in">zip</span></span><br><span class="line">!unzip data.<span class="built_in">zip</span></span><br><span class="line">!rm -rf data.<span class="built_in">zip</span></span><br></pre></td></tr></table></figure>
<p>AlexNet的输入是<code>[227,227,3]</code>，因此要对输入进行预处理，使得符合规范，同时还需要进行贴标签(dog=1,cat=0)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers,models</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">125</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span>(<span class="params">img_path,size=(<span class="params"><span class="number">227</span>,<span class="number">227</span></span>)</span>):</span></span><br><span class="line">    label = tf.constant(<span class="number">0</span>,tf.int8) <span class="keyword">if</span> tf.strings.regex_full_match(img_path,<span class="string">&quot;.*cat.*&quot;</span>) \</span><br><span class="line">            <span class="keyword">else</span> tf.constant(<span class="number">1</span>,tf.int8)</span><br><span class="line">    img = tf.io.read_file(img_path)</span><br><span class="line">    img = tf.image.decode_jpeg(img) </span><br><span class="line">    img = tf.image.resize(img,size)/<span class="number">255.0</span>  <span class="comment">#转float要归一</span></span><br><span class="line">    <span class="keyword">return</span>(img,label)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用并行化预处理num_parallel_calls 和预存数据prefetch来提升性能</span></span><br><span class="line"><span class="comment"># num_parallel_calls=tf.data.experimental.AUTOTUNE 根据CPU动态处理 未指定就顺序处理</span></span><br><span class="line"><span class="comment">#shuffle 随机重排</span></span><br><span class="line"><span class="comment">#batch()一次出多少</span></span><br><span class="line"><span class="comment">#prefetch 提前取出多少批batch</span></span><br><span class="line">ds_train = tf.data.Dataset.list_files(<span class="string">&quot;./data/train/*.jpg&quot;</span>) \</span><br><span class="line">           .<span class="built_in">map</span>(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .shuffle(buffer_size = <span class="number">1000</span>).batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)  </span><br><span class="line"></span><br><span class="line">ds_test = tf.data.Dataset.list_files(<span class="string">&quot;./data/test/*.jpg&quot;</span>) \</span><br><span class="line">           .<span class="built_in">map</span>(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \</span><br><span class="line">           .batch(BATCH_SIZE) \</span><br><span class="line">           .prefetch(tf.data.experimental.AUTOTUNE)  </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看训练样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))  <span class="comment">#8*8 inch</span></span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ds_train.unbatch().take(<span class="number">9</span>)):  <span class="comment">#enumerate 组合成索引、数据迭代</span></span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    ax.set_title(<span class="string">&quot;label = %d&quot;</span>%label)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/DogsVsCats/output_7_0.svg" alt="" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> ds_train.take(<span class="number">1</span>):</span><br><span class="line">    print(x.shape,y.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(125, 227, 227, 3) (125,)
</code></pre>
<h2 id="2-定义模型"><a class="markdownIt-Anchor" href="#2-定义模型"></a> 2、定义模型</h2>
<p>AlexNet相较于以前的网络主要引入了Relu激活函数、DropOut层和lrn局部响应</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># LRN层</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LRN</span>(<span class="params">layers.Layer</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LRN, self).__init__()</span><br><span class="line">        self.depth_radius=<span class="number">2</span></span><br><span class="line">        self.bias=<span class="number">1</span></span><br><span class="line">        self.alpha=<span class="number">1e-4</span></span><br><span class="line">        self.beta=<span class="number">0.75</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.lrn(x,depth_radius=self.depth_radius,</span><br><span class="line">                         bias=self.bias,alpha=self.alpha,</span><br><span class="line">                         beta=self.beta)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.keras.backend.clear_session() <span class="comment">#清空会话</span></span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">96</span>,kernel_size=(<span class="number">11</span>,<span class="number">11</span>),activation=<span class="string">&#x27;relu&#x27;</span>,strides=(<span class="number">4</span>,<span class="number">4</span>),input_shape=(<span class="number">227</span>,<span class="number">227</span>,<span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(LRN())</span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>,kernel_size=(<span class="number">5</span>,<span class="number">5</span>),activation=<span class="string">&#x27;relu&#x27;</span>,strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(LRN())</span><br><span class="line">model.add(layers.Conv2D(<span class="number">384</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>),activation=<span class="string">&#x27;relu&#x27;</span>,strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">384</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>),activation=<span class="string">&#x27;relu&#x27;</span>,strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">256</span>,kernel_size=(<span class="number">3</span>,<span class="number">3</span>),activation=<span class="string">&#x27;relu&#x27;</span>,strides=(<span class="number">1</span>,<span class="number">1</span>),padding=<span class="string">&#x27;same&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>),strides=(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten()) <span class="comment"># 不要忘记有一层降为一维</span></span><br><span class="line">model.add(layers.Dense(<span class="number">4096</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">4096</span>,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.25</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">2</span>,activation=<span class="string">&#x27;softmax&#x27;</span>))  <span class="comment">#真实的AlexNet会分成1000类，此处分2类 sofmax来计算概率</span></span><br><span class="line"></span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 55, 55, 96)        34944     
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 27, 27, 96)        0         
_________________________________________________________________
lrn (LRN)                    (None, 27, 27, 96)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 27, 27, 256)       614656    
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0         
_________________________________________________________________
lrn_1 (LRN)                  (None, 13, 13, 256)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 13, 13, 384)       885120    
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 13, 13, 384)       1327488   
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 13, 13, 256)       884992    
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         
_________________________________________________________________
flatten (Flatten)            (None, 9216)              0         
_________________________________________________________________
dense (Dense)                (None, 4096)              37752832  
_________________________________________________________________
dropout (Dropout)            (None, 4096)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 8194      
=================================================================
Total params: 58,289,538
Trainable params: 58,289,538
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h2 id="3-训练模型"><a class="markdownIt-Anchor" href="#3-训练模型"></a> 3、训练模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">0.00001</span>),</span><br><span class="line">        loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">        metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line"></span><br><span class="line">checkpoint_save_path=<span class="string">&#x27;./checkpoint/AlexNet/AlexNet-&#123;epoch:02d&#125;.ckpt&#x27;</span></span><br><span class="line"><span class="comment">#tf生成ckpt文件时会同步生成索引表，那么通过判断是否有索引表，就知道有没有保存过参数。</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(checkpoint_save_path+<span class="string">&#x27;.index&#x27;</span>):</span><br><span class="line">    print(<span class="string">&#x27;----------------load the model--------------&#x27;</span>)</span><br><span class="line">    model.load_weights(checkpoint_save_path)</span><br><span class="line">cp_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,</span><br><span class="line">                            save_weights_only=<span class="literal">True</span>,</span><br><span class="line">                            save_best_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">history = model.fit(ds_train,epochs= <span class="number">20</span>,validation_data=ds_test,</span><br><span class="line">                    callbacks = [cp_callback],workers = <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
184/184 [==============================] - 40s 219ms/step - loss: 0.6630 - accuracy: 0.5944 - val_loss: 0.6141 - val_accuracy: 0.6615
Epoch 2/20
184/184 [==============================] - 40s 220ms/step - loss: 0.5970 - accuracy: 0.6754 - val_loss: 0.5586 - val_accuracy: 0.7040
Epoch 3/20
184/184 [==============================] - 40s 219ms/step - loss: 0.5426 - accuracy: 0.7244 - val_loss: 0.5088 - val_accuracy: 0.7505
Epoch 4/20
184/184 [==============================] - 40s 219ms/step - loss: 0.4946 - accuracy: 0.7598 - val_loss: 0.4840 - val_accuracy: 0.7735
Epoch 5/20
184/184 [==============================] - 40s 220ms/step - loss: 0.4545 - accuracy: 0.7852 - val_loss: 0.4469 - val_accuracy: 0.8055
Epoch 6/20
184/184 [==============================] - 40s 220ms/step - loss: 0.4207 - accuracy: 0.8064 - val_loss: 0.4180 - val_accuracy: 0.8100
Epoch 7/20
184/184 [==============================] - 40s 219ms/step - loss: 0.3991 - accuracy: 0.8193 - val_loss: 0.4016 - val_accuracy: 0.8250
Epoch 8/20
184/184 [==============================] - 41s 220ms/step - loss: 0.3788 - accuracy: 0.8309 - val_loss: 0.3908 - val_accuracy: 0.8315
Epoch 9/20
184/184 [==============================] - 40s 219ms/step - loss: 0.3587 - accuracy: 0.8427 - val_loss: 0.3733 - val_accuracy: 0.8375
Epoch 10/20
184/184 [==============================] - 38s 207ms/step - loss: 0.3387 - accuracy: 0.8539 - val_loss: 0.3860 - val_accuracy: 0.8345
Epoch 11/20
184/184 [==============================] - 40s 220ms/step - loss: 0.3273 - accuracy: 0.8576 - val_loss: 0.3498 - val_accuracy: 0.8475
Epoch 12/20
184/184 [==============================] - 38s 207ms/step - loss: 0.3064 - accuracy: 0.8676 - val_loss: 0.3671 - val_accuracy: 0.8380
Epoch 13/20
184/184 [==============================] - 38s 208ms/step - loss: 0.2931 - accuracy: 0.8762 - val_loss: 0.3501 - val_accuracy: 0.8495
Epoch 14/20
184/184 [==============================] - 40s 220ms/step - loss: 0.2737 - accuracy: 0.8864 - val_loss: 0.3361 - val_accuracy: 0.8540
Epoch 15/20
184/184 [==============================] - 40s 219ms/step - loss: 0.2694 - accuracy: 0.8890 - val_loss: 0.3311 - val_accuracy: 0.8585
Epoch 16/20
184/184 [==============================] - 38s 208ms/step - loss: 0.2557 - accuracy: 0.8938 - val_loss: 0.3388 - val_accuracy: 0.8565
Epoch 17/20
184/184 [==============================] - 40s 219ms/step - loss: 0.2362 - accuracy: 0.9030 - val_loss: 0.3160 - val_accuracy: 0.8680
Epoch 18/20
184/184 [==============================] - 38s 207ms/step - loss: 0.2184 - accuracy: 0.9106 - val_loss: 0.3692 - val_accuracy: 0.8515
Epoch 19/20
184/184 [==============================] - 38s 207ms/step - loss: 0.2085 - accuracy: 0.9160 - val_loss: 0.3198 - val_accuracy: 0.8710
Epoch 20/20
184/184 [==============================] - 41s 222ms/step - loss: 0.1909 - accuracy: 0.9239 - val_loss: 0.3019 - val_accuracy: 0.8705
</code></pre>
<h2 id="4-评估模型"><a class="markdownIt-Anchor" href="#4-评估模型"></a> 4、评估模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_metric</span>(<span class="params">history, metric</span>):</span></span><br><span class="line">    train_metrics = history.history[metric]</span><br><span class="line">    val_metrics = history.history[<span class="string">&#x27;val_&#x27;</span>+metric]</span><br><span class="line">    epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(train_metrics) + <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, train_metrics, <span class="string">&#x27;bo--&#x27;</span>)</span><br><span class="line">    plt.plot(epochs, val_metrics, <span class="string">&#x27;ro-&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and validation &#x27;</span>+ metric)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Epochs&quot;</span>)</span><br><span class="line">    plt.ylabel(metric)</span><br><span class="line">    plt.legend([<span class="string">&quot;train_&quot;</span>+metric, <span class="string">&#x27;val_&#x27;</span>+metric])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">&quot;loss&quot;</span>) <span class="comment">#结果看可以再练练</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/DogsVsCats/output_16_0.svg" alt="" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plot_metric(history,<span class="string">&quot;accuracy&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/images/DogsVsCats/output_17_0.svg" alt="" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#可以使用evaluate对数据进行评估</span></span><br><span class="line">val_loss,val_accuracy = model.evaluate(ds_test,workers=<span class="number">4</span>)</span><br><span class="line">print(val_loss,val_accuracy)</span><br></pre></td></tr></table></figure>
<pre><code>16/16 [==============================] - 2s 144ms/step - loss: 0.3019 - accuracy: 0.8705
0.3018629252910614 0.8705000281333923
</code></pre>
<h2 id="5-使用模型"><a class="markdownIt-Anchor" href="#5-使用模型"></a> 5、使用模型</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perpareImage</span>(<span class="params">img,sizes=(<span class="params">-<span class="number">1</span>,<span class="number">227</span>,<span class="number">227</span>,<span class="number">3</span></span>)</span>):</span></span><br><span class="line">  img = tf.reshape(img,shape=sizes)</span><br><span class="line">  <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ds_test.unbatch().take(<span class="number">1</span>)):</span><br><span class="line">  img=tf.reshape(img,shape=(-<span class="number">1</span>,<span class="number">227</span>,<span class="number">227</span>,<span class="number">3</span>))</span><br><span class="line">  p=model.predict(img)</span><br><span class="line">  print(p)</span><br><span class="line">  p=tf.argmax(p,<span class="number">1</span>)</span><br><span class="line">  print(p)</span><br><span class="line">  print(p.shape)</span><br></pre></td></tr></table></figure>
<pre><code>[[0.91607434 0.08392565]]
tf.Tensor([0], shape=(1,), dtype=int64)
(1,)
</code></pre>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">&#x27;svg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看训练样本</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt </span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))  <span class="comment">#8*8 inch</span></span><br><span class="line"><span class="keyword">for</span> i,(img,label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(ds_test.unbatch().take(<span class="number">9</span>)):  <span class="comment">#enumerate 组合成索引、数据迭代</span></span><br><span class="line">    ax=plt.subplot(<span class="number">3</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    ax.imshow(img.numpy())</span><br><span class="line">    predict = model.predict(perpareImage(img))</span><br><span class="line">    ax.set_title(<span class="string">&quot;predict=%d,label=%d&quot;</span>%(tf.argmax(predict,<span class="number">1</span>),label))</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([]) </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/images/DogsVsCats/output_22_0.svg" alt="" /></p>
<h2 id="6-保存模型"><a class="markdownIt-Anchor" href="#6-保存模型"></a> 6、保存模型</h2>
<p>tensorflow原生保存</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 保存模型结构与模型参数到文件,该方式保存的模型具有跨平台性便于部署</span></span><br><span class="line"></span><br><span class="line">model.save(<span class="string">&#x27;./model/tf_AlexNet_Model&#x27;</span>, save_format=<span class="string">&quot;tf&quot;</span>)</span><br><span class="line">print(<span class="string">&#x27;export saved model.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">model_loaded = tf.keras.models.load_model(<span class="string">&#x27;./model/tf_AlexNet_Model&#x27;</span>)</span><br><span class="line">model_loaded.evaluate(ds_test)</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: ./model/tf_AlexNet_Model/assets
export saved model.
16/16 [==============================] - 2s 147ms/step - loss: 0.3019 - accuracy: 0.5155





[0.301862895488739, 0.515500009059906]
</code></pre>
<h2 id="repo"><a class="markdownIt-Anchor" href="#repo"></a> repo</h2>
<p><a href="https://github.com/qxdn/DogsVsCats">DogsVsCats</a></p>
<h2 id="后记"><a class="markdownIt-Anchor" href="#后记"></a> 后记</h2>
<p>踩的坑有点点多，比如搭建网络时候忘记摊平，预测的时候因为形状不对卡了半天，CPU训练的时候loss和准确度不变，差点以为失败了，不过最后看来还算成功吧。赞美Google Colab。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
        <tag>机器学习</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title>升级到WSL2</title>
    <url>/2020/10/23/upgradeWSL2/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>因为一些个人原因，需要将原先的WSL1升级到WSL2，参考<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-win10">微软文档</a>，记录踩坑</p>
<h2 id="一-检查版本"><a class="markdownIt-Anchor" href="#一-检查版本"></a> 一、检查版本</h2>
<p>对于x64系统目前的要求是<code>1903</code>或者更高,<code>Build 18362</code>或者更高。使用<code>win+R</code>输入<code>winver</code>来检查自己的版本<br />
<img src="/images/post-upgradeWSL/version.png" alt="version" /></p>
<h2 id="二-启动虚拟机功能"><a class="markdownIt-Anchor" href="#二-启动虚拟机功能"></a> 二、启动虚拟机功能</h2>
<p>管理员模式启动powershell输入</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">dism.exe /online /<span class="built_in">enable-feature</span> /featurename:VirtualMachinePlatform /all /norestart</span><br></pre></td></tr></table></figure>
<p><img src="/images/post-upgradeWSL/enableVMFeature.png" alt="启动虚拟机功能" /><br />
<strong>然后重启电脑</strong></p>
<h2 id="三-下载linux内核升级包"><a class="markdownIt-Anchor" href="#三-下载linux内核升级包"></a> 三、下载Linux内核升级包</h2>
<p>下载最新<a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi">升级包</a><br />
<img src="/images/post-upgradeWSL/package.png" alt="package" /></p>
<h2 id="四-设置wsl2作为默认版本"><a class="markdownIt-Anchor" href="#四-设置wsl2作为默认版本"></a> 四、设置WSL2作为默认版本</h2>
<p>管理员powershell中运行</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">wsl -<span class="literal">-set</span><span class="literal">-default</span><span class="literal">-version</span> <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="五-转换wsl"><a class="markdownIt-Anchor" href="#五-转换wsl"></a> 五、转换WSL</h2>
<p>使用该指令查看wsl状态</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">wsl <span class="literal">-l</span> <span class="literal">-v</span></span><br></pre></td></tr></table></figure>
<p>我的输出</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">  NAME      STATE           VERSION</span><br><span class="line">* Ubuntu    Stopped         <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>转换WSL</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">wsl -<span class="literal">-set</span><span class="literal">-version</span> Ubuntu <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">正在进行转换，这可能需要几分钟时间...</span><br><span class="line">有关与 WSL <span class="number">2</span> 的主要区别的信息，请访问 https://aka.ms/wsl2</span><br><span class="line">请启用虚拟机平台 Windows 功能并确保在 BIOS 中启用虚拟化。</span><br><span class="line">有关信息，请访问 https://aka.ms/wsl2<span class="literal">-install</span></span><br></pre></td></tr></table></figure>
<p>这里在我搜索了<a href="https://github.com/microsoft/WSL/issues/5363">issue</a>后发现主要还是Hyper-V、虚拟平台等问题，可以试一试管理员模式运行以下指令</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">DISM /Online /<span class="built_in">Enable-Feature</span> /All /FeatureName:Microsoft<span class="literal">-Hyper</span><span class="literal">-V</span></span><br><span class="line">dism.exe /online /<span class="built_in">enable-feature</span> /featurename:Microsoft<span class="literal">-Windows</span><span class="literal">-Subsystem</span><span class="literal">-Linux</span> /all /norestart</span><br><span class="line">dism.exe /online /<span class="built_in">enable-feature</span> /featurename:VirtualMachinePlatform /all /norestart</span><br><span class="line">bcdedit /<span class="built_in">set</span> hypervisorlaunchtype auto</span><br></pre></td></tr></table></figure>
<p>最主要的应该是</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">DISM /Online /<span class="built_in">Enable-Feature</span> /All /FeatureName:Microsoft<span class="literal">-Hyper</span><span class="literal">-V</span></span><br><span class="line">bcdedit /<span class="built_in">set</span> hypervisorlaunchtype auto</span><br></pre></td></tr></table></figure>
<p>重新转换<br />
<img src="/images/post-upgradeWSL/converted.png" alt="转换完成" /></p>
<h2 id="后记"><a class="markdownIt-Anchor" href="#后记"></a> 后记</h2>
<p>VMware老版本与Hyper-V冲突，得找个时间去更新到最新版本兼容</p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>Linux</tag>
        <tag>WSL</tag>
      </tags>
  </entry>
  <entry>
    <title>毕业设计（一）环境搭建</title>
    <url>/2021/01/18/installpytorch/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>体验过CPU和GPU速度之后，就不会再想使用CPU版本的机器学习框架进行训练。</p>
<h2 id="安装cuda"><a class="markdownIt-Anchor" href="#安装cuda"></a> 安装CUDA</h2>
<p>第一步就是要完成cuda的安装，进入<code>NIVIDA控制面板</code>-&gt;<code>系统信息</code>-&gt;<code>组件</code>，查看<code>3D设置</code>中的<code>NVCUDA64.DLL</code>，这里显示的就是当前显卡驱动所支持的CUDA信息，所要安装的CUDA版本不得大于这里显示的版本。然后前往<a href="https://developer.nvidia.com/zh-cn/cuda-downloads">CUDA官网</a>下载CUDA。建议在下载之前先去其他一些网址看一下支持的CUDA版本，以免到时候重新安装</p>
<ul>
<li><a href="https://tensorflow.google.cn/install/source_windows">tensorflow</a> (换成English可以看最新支持)</li>
<li><a href="https://pytorch.org/get-started/locally/">pytorch</a></li>
</ul>
<p><img src="/images/GraduationProject/GPUInfo.png" alt="GPU信息" /></p>
<p>如果发现自己安装的版本没有已经构建好的版本还有两种方法。</p>
<ul>
<li>自行从源码编译，这点不推荐，想起来我之间自己在树莓派上面编译opencv的经历颜文字(ಥ _ ಥ)，强烈不推荐。</li>
<li>卸载重装CUDA</li>
</ul>
<p>重装CUDA比较简单,见下图中的NVIDIA应用除了<code>NVIDIA的图形驱动程序</code>和<code>NVIDIA Physx系统软件</code>都卸载就行。如果下不动的就使用迅雷11，新版迅雷配合网盘简直神一般的体验。</p>
<p><img src="/images/GraduationProject/uninstall.png" alt="GPU信息" /></p>
<p>安装完成后可以在命令行里面使用<code>nvcc -V</code>看看结果。</p>
<h2 id="cudnn安装"><a class="markdownIt-Anchor" href="#cudnn安装"></a> cuDNN安装</h2>
<p>cuDNN安装比较简单，首先进入<a href="https://developer.nvidia.com/rdp/cudnn-archive">官网</a>，选择你安装的CUDA的对应版本即可。下载也需要进行注册，填一个调查问卷，但是因为我实在进不去，就使用迅雷接管下载链接之间下载。将解压后的<code>bin</code>、<code>include</code>、<code>lib</code>三个文件复制进入<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0</code>即可。我安装的版本是<code>8.0.4</code></p>
<h2 id="pytorch安装"><a class="markdownIt-Anchor" href="#pytorch安装"></a> pytorch安装</h2>
<p>听闻pytorch容易调试，且搭建网络比较快，而我此前只是用过tensorflow2.0中的keras进行过搭建，因此本次毕设初步打算使用pytorch作为工具，也可能不用呢(￣y▽,￣)╭ 。使用pytorch推荐的conda安装方法</p>
<p><code>conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch</code></p>
<p>这里我是使用清华镜像源的，是否要加<code>-c pytorch</code>还是看你的<code>.condarc</code>怎么写的，如果写了<code>custom_channels</code>且里面的<code>pytorch</code>也是用了清华源的可以加，没有的话就去掉<code>-c pytorch</code>，然而我镜像源也下不了（吐了），发现浏览器也下不了，就只能用迅雷接管下载，然后本地安装。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#本地安装</span></span><br><span class="line">conda install --use-local cudatoolkit-11.0.221-h74a9793_0.conda</span><br><span class="line">conda install --use-local pytorch-1.7.1-py3.8_cuda110_cudnn8_0.tar.bz2</span><br></pre></td></tr></table></figure>
<p>然后进行测试<br />
<img src="/images/GraduationProject/verify.png" alt="verify" /></p>
<h2 id="后记"><a class="markdownIt-Anchor" href="#后记"></a> 后记</h2>
<p>环境的搭建就到此为止，安装并不难，只是我没想到我尽然会连镜像源都下不动😩。后续视情况可能会使用GPU服务器，但是现在先在本地收集数据集和预先学习。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>毕业设计</tag>
        <tag>yolo</tag>
      </tags>
  </entry>
  <entry>
    <title>win10家庭版下安装docker</title>
    <url>/2021/03/18/installDocker/</url>
    <content><![CDATA[<p>因为装软件、装环境老是折腾，就开始思考各个生产环境之间隔离或者直接使用他人配好的环境。因而想到了人们常说的docker。因此打算安装来体验一下</p>
<span id="more"></span>
<h1 id="系统要求"><a class="markdownIt-Anchor" href="#系统要求"></a> 系统要求</h1>
<p>根据官方文档，<code>Docker Desktop on Windows</code>需要win10专业版、企业版或者教育版(Build 17134 or later)。而我的电脑为win10家庭版，因此选择另一套方案。</p>
<p>win10家庭版的系统要求如下</p>
<ul>
<li>win10 v1903版本或者更高</li>
<li>开启WSL2
<ul>
<li>64位处理器且有SLAT指令</li>
<li>4G内存</li>
<li>BIOS开启虚拟化</li>
</ul>
</li>
<li>下载并升级<a href="https://docs.microsoft.com/zh-cn/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package">linux内核包</a></li>
</ul>
<h1 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h1>
<ol>
<li>下载<a href="https://desktop.docker.com/win/stable/Docker%20Desktop%20Installer.exe">Docker Desktop Installer</a>并双击运行</li>
<li>确保WSL2那个特性打开<br />
<img src="/images/installDocker/dockerInstallerConfig.png" alt="enable WSL2" /></li>
<li>跟着安装帮助即可</li>
</ol>
<h2 id="错误"><a class="markdownIt-Anchor" href="#错误"></a> 错误</h2>
<p>我就知道安装没有这么容易，果然碰到了问题。<br />
<img src="/images/installDocker/error.png" alt="error" /><br />
通过<code>everything</code>我找到了<code>install-log.txt</code>。然而里面的关键信息就这么点。明明已经用管理员运行了，但是还是未授权。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception type: System.Exception, Exception message: Component CommunityInstaller.AutoStartAction failed: 尝试执行未经授权的操作。, StackTrace:</span><br><span class="line">   在 CommunityInstaller.InstallWorkflow.&lt;DoHandleD4WPackageAsync&gt;d__29.MoveNext()</span><br><span class="line">--- 引发异常的上一位置中堆栈跟踪的末尾 ---</span><br><span class="line">   在 System.Runtime.ExceptionServices.ExceptionDispatchInfo.Throw()</span><br><span class="line">   在 System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task)</span><br><span class="line">   在 CommunityInstaller.InstallWorkflow.&lt;DoProcessAsync&gt;d__23.MoveNext()</span><br></pre></td></tr></table></figure>
<p>就在我一筹莫展的时候，我看到了一个服务器部署win10服务未授权解决的文章，说是360阻断了。于是我尝试将我的联想电脑管家关闭，果然安装成功了。<br />
<img src="/images/installDocker/docker.png" alt="docker" /></p>
<h1 id="参考档案"><a class="markdownIt-Anchor" href="#参考档案"></a> 参考档案</h1>
<p><a href="https://docs.docker.com/docker-for-windows/install/">docker for win10 pro</a></p>
<p><a href="https://docs.docker.com/docker-for-windows/install-windows-home/">docker for win10 home</a></p>
<p><a href="https://qianxu.run/2020/10/23/upgradeWSL2/">WSL2安装</a></p>
<p><a href="https://blog.csdn.net/qq736150416/article/details/80137675">灵感</a></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>MinGW编译C++无法输出string</title>
    <url>/2021/03/19/mingw64problem/</url>
    <content><![CDATA[<p>在调试程序的时候遇到了一个奇葩的问题，C++无法使用string。最后发现居然是MinGW64的问题</p>
<span id="more"></span>
<h1 id="起因"><a class="markdownIt-Anchor" href="#起因"></a> 起因</h1>
<p>在帮助别人调试<a href="https://github.com/hediet/vscode-debug-visualizer">vscode-debug-visualizer</a>这个插件的时候，遇到了无法出结果的问题。经过一系列定位后发现，居然是使用了string就无法运行，而编译不会出错。</p>
<h1 id="测试程序"><a class="markdownIt-Anchor" href="#测试程序"></a> 测试程序</h1>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//main.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// visualize `myGraphJson`!</span></span><br><span class="line">    <span class="built_in">string</span> myGraphJson = <span class="string">&quot;&#123;\&quot;kind\&quot;:&#123;\&quot;graph\&quot;:true&#125;,&quot;</span></span><br><span class="line">        <span class="string">&quot;\&quot;nodes\&quot;:[&#123;\&quot;id\&quot;:\&quot;1\&quot;&#125;,&#123;\&quot;id\&quot;:\&quot;2\&quot;&#125;],&quot;</span></span><br><span class="line">        <span class="string">&quot;\&quot;edges\&quot;:[&#123;\&quot;from\&quot;:\&quot;1\&quot;,\&quot;to\&quot;:\&quot;2\&quot;&#125;]&#125;&quot;</span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; myGraphJson;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="错误现象"><a class="markdownIt-Anchor" href="#错误现象"></a> 错误现象</h1>
<p>使用编译指令如下</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">g++ <span class="literal">-g</span> main.c <span class="literal">-o</span> main.exe</span><br></pre></td></tr></table></figure>
<p>结果正常，运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.\main.exe</span><br></pre></td></tr></table></figure>
<p>输出为空</p>
<p>使用gdb进行debug。错误如下</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">ERROR: Unable to <span class="built_in">start</span> debugging. Unexpected GDB output from command <span class="string">&quot;-exec-run&quot;</span>. During startup program exited with code <span class="number">0</span>xc0000139.</span><br></pre></td></tr></table></figure>
<h1 id="解决方案"><a class="markdownIt-Anchor" href="#解决方案"></a> 解决方案</h1>
<p>在<a href="https://stackoverflow.com/questions/18668003/the-procedure-entry-point-gxx-personality-v0-could-not-be-located">stackoverflow</a>上找到解决方案。</p>
<p>从<code>MinGW</code>的安装目录里面找到<code>bin</code>文件夹。将里面的<code>libstdc++-6.dll</code>拷贝到工作目录。目录结构如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">+ .vscode</span><br><span class="line">    launch.json</span><br><span class="line">    tasks.json</span><br><span class="line">libstdc++-6.dll</span><br><span class="line">main.cpp</span><br></pre></td></tr></table></figure>
<p>再次进行编译、运行</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">g++ <span class="literal">-g</span> main.c <span class="literal">-o</span> main.exe</span><br><span class="line">.\main.exe</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">&#123;<span class="string">&quot;kind&quot;</span>:&#123;<span class="string">&quot;graph&quot;</span>:true&#125;,<span class="string">&quot;nodes&quot;</span>:[&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;1&quot;</span>&#125;,&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;2&quot;</span>&#125;],<span class="string">&quot;edges&quot;</span>:[&#123;<span class="string">&quot;from&quot;</span>:<span class="string">&quot;1&quot;</span>,<span class="string">&quot;to&quot;</span>:<span class="string">&quot;2&quot;</span>&#125;]&#125;</span><br></pre></td></tr></table></figure>
<p>gdb的debug结果正常</p>
<h1 id="吐槽"><a class="markdownIt-Anchor" href="#吐槽"></a> 吐槽</h1>
<p>这个问题在7年前就被提出了，居然现在还有。MinGW真是坑啊，用了环境变量也没用。</p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>安装tensorflow-gpu版本</title>
    <url>/2021/03/26/install-tensorflow-gpu/</url>
    <content><![CDATA[<p>对于个人而言，还是比较习惯使用tensorflow的keras</p>
<span id="more"></span>
<h1 id="安装cuda和cudnn"><a class="markdownIt-Anchor" href="#安装cuda和cudnn"></a> 安装cuda和cudnn</h1>
<p>安装方法可以见<a href="https://qianxu.run/2021/01/18/installpytorch/">pytorch的安装</a>，对于tensorflow已有的gpu支持可以见<a href="https://tensorflow.google.cn/install/source_windows#gpu">此</a>。不提前装cuda和cudnn的方法见<a href="#%E4%BD%BF%E7%94%A8conda%E5%AE%89%E8%A3%85cuda%E5%92%8Ccudnn">使用conda安装cuda和cudnn</a></p>
<h1 id="安装tensorflow-gpu"><a class="markdownIt-Anchor" href="#安装tensorflow-gpu"></a> 安装tensorflow-gpu</h1>
<p>由于我刚好符合<code>tensorflow-gpu 2.4.0</code>，而<code>conda</code>还没有这个包，因此使用<code>pip</code>进行安装</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install tensorflow-gpu==<span class="number">2.4</span><span class="number">.0</span> -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>
<p>镜像源可能下载速度还是不行，可以直接本地安装</p>
<h1 id="验证gpu"><a class="markdownIt-Anchor" href="#验证gpu"></a> 验证gpu</h1>
<p>运行代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">print(tf.__version__)</span><br><span class="line">print(tf.config.list_physical_devices())</span><br></pre></td></tr></table></figure>
<p>输出结果如下</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">35.277510</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cudart64_110.dll2.<span class="number">4.0</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.259583</span>: I tensorflow/compiler/jit/xla_cpu_device.cc:<span class="number">41</span>] Not creating XLA devices, tf_xla_enable_xla_devices not <span class="built_in">set</span></span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.261539</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library nvcuda.dll      </span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.307535</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">1720</span>] Found device <span class="number">0</span> with properties: </span><br><span class="line">pciBusID: <span class="number">0000</span>:<span class="number">01</span>:<span class="number">00.0</span> name: GeForce GTX <span class="number">1050</span> computeCapability: <span class="number">6.1</span></span><br><span class="line">coreClock: <span class="number">1.493</span>GHz coreCount: <span class="number">5</span> deviceMemorySize: <span class="number">2.00</span>GiB deviceMemoryBandwidth: <span class="number">104.43</span>GiB/s</span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.308404</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cudart64_110.dll2021<span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.322068</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cublas64_11.dll </span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.322414</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cublasLt64_11.dll</span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.329903</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cufft64_10.dll  </span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.332538</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library curand64_10.dll </span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.347776</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cusolver64_10.dll</span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.357722</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cusparse64_11.dll</span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.362238</span>: I tensorflow/stream_executor/platform/default/dso_loader.cc:<span class="number">49</span>] Successfully opened dynamic library cudnn64_8.dll   </span><br><span class="line"><span class="number">2021</span><span class="literal">-03</span><span class="literal">-26</span> <span class="number">17</span>:<span class="number">00</span>:<span class="number">39.362701</span>: I tensorflow/core/common_runtime/gpu/gpu_device.cc:<span class="number">1862</span>] Adding visible gpu devices: <span class="number">0</span></span><br><span class="line">[<span class="type">PhysicalDevice</span>(<span class="type">name</span>=<span class="string">&#x27;/physical_device:CPU:0&#x27;</span>, <span class="type">device_type</span>=<span class="string">&#x27;CPU&#x27;</span>), <span class="type">PhysicalDevice</span>(<span class="type">name</span>=<span class="string">&#x27;/physical_device:GPU:0&#x27;</span>, <span class="type">device_type</span>=<span class="string">&#x27;GPU&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<h1 id="使用conda安装cuda和cudnn"><a class="markdownIt-Anchor" href="#使用conda安装cuda和cudnn"></a> 使用conda安装cuda和cudnn</h1>
<p>事实上使用conda的时候不提前装cuda和cudnn也是可以的。<br />
执行以下代码即可。注意对比tensorflow的gpu支持版本，见<a href="https://tensorflow.google.cn/install/source_windows#gpu">此</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda install cudatoolkit=<span class="number">10.1</span> cudnn=<span class="number">7.6</span><span class="number">.5</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>tmux vs screen</title>
    <url>/2021/03/28/tmux-vs-screen/</url>
    <content><![CDATA[<p>本文将对tmux与screen命令的使用进行比较，并简单的进行对比。</p>
<p>封面图片来源《妄想破绽》</p>
<span id="more"></span>
<h1 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h1>
<p>你是否有过在SSH连接远程终端的时候，需要同时后台运行多个程序，抑或是需要开启多个终端进行观察。那么你需要一个终端复用程序，比如<code>tmux</code>或者<code>screen</code>。<code>tmux</code>和<code>screen</code>命令差不多，都是终端复用程序，你还可以在SSH断开后重新连接端口继续执行命令。</p>
<h1 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h1>
<p>如果你的Ubuntu没有<code>tmux</code>或者<code>screen</code>，那么你可以使用如下命令进行安装</p>
<p><code>tmux</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install tmux</span><br></pre></td></tr></table></figure>
<p><code>screen</code>:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install screen</span><br></pre></td></tr></table></figure>
<h1 id="使用"><a class="markdownIt-Anchor" href="#使用"></a> 使用</h1>
<h2 id="tmux"><a class="markdownIt-Anchor" href="#tmux"></a> tmux</h2>
<h3 id="简单创建"><a class="markdownIt-Anchor" href="#简单创建"></a> 简单创建</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux</span><br></pre></td></tr></table></figure>
<p><img src="/images/tmux-vs-screen/simple-tmux.png" alt="效果图" /></p>
<p>[0]0:zsh* : 意味着现在是session name为0，session number为0，环境为zsh，*意味着当前窗口</p>
<p>“DESKTOP-7A4A8RD” : hostname</p>
<p>18:59 28-Mar-21 : 当前时间</p>
<h3 id="创建一个有名字的session"><a class="markdownIt-Anchor" href="#创建一个有名字的session"></a> 创建一个有名字的session</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux new -s &lt;session-name&gt;</span><br></pre></td></tr></table></figure>
<p>此时可以看到原本[0]里面的0变成了设置的session-name</p>
<h3 id="竖直分离窗口"><a class="markdownIt-Anchor" href="#竖直分离窗口"></a> 竖直分离窗口</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>%</code><br />
<img src="/images/tmux-vs-screen/tmux-vertically.png" alt="效果图" /></p>
<h3 id="水平分离窗口"><a class="markdownIt-Anchor" href="#水平分离窗口"></a> 水平分离窗口</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>&quot;</code><br />
<img src="/images/tmux-vs-screen/tmux-horizontally.png" alt="效果图" /></p>
<h3 id="展示会话编号"><a class="markdownIt-Anchor" href="#展示会话编号"></a> 展示会话编号</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>q</code><br />
<img src="/images/tmux-vs-screen/tmux-number.png" alt="效果图" /></p>
<h3 id="切换窗口"><a class="markdownIt-Anchor" href="#切换窗口"></a> 切换窗口</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>o</code></p>
<h3 id="关闭窗口"><a class="markdownIt-Anchor" href="#关闭窗口"></a> 关闭窗口</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>x</code>或者<code>ctrl+d</code></p>
<h3 id="列出已有tmux"><a class="markdownIt-Anchor" href="#列出已有tmux"></a> 列出已有tmux</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>s</code><br />
<img src="/images/tmux-vs-screen/tmux-list.png" alt="效果图" /></p>
<h3 id="分离会话"><a class="markdownIt-Anchor" href="#分离会话"></a> 分离会话</h3>
<p>在<code>tmux</code>窗口中按下<code>ctrl+b</code>然后再按下<code>d</code>或者</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux detach</span><br></pre></td></tr></table></figure>
<h3 id="重连会话"><a class="markdownIt-Anchor" href="#重连会话"></a> 重连会话</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux attach -t &lt;session-name&gt;</span><br></pre></td></tr></table></figure>
<h3 id="杀死会话"><a class="markdownIt-Anchor" href="#杀死会话"></a> 杀死会话</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tmux kill-session -t &lt;session-name&gt;</span><br></pre></td></tr></table></figure>
<h2 id="screen"><a class="markdownIt-Anchor" href="#screen"></a> screen</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s &lt;shell&gt;][-S &lt;作业名称&gt;]</span><br></pre></td></tr></table></figure>
<p>wsl中使用<code>screen</code>提示权限不足可以参考本文末尾的<a href="#wsl%E4%B8%AD%E6%8F%90%E7%A4%BAscreen%E9%9C%80%E8%A6%81root%E6%9D%83%E9%99%90%E8%A7%A3%E5%86%B3">wsl中提示screen需要root权限解决</a></p>
<h3 id="创建简单窗口"><a class="markdownIt-Anchor" href="#创建简单窗口"></a> 创建简单窗口</h3>
<p>在<code>screen</code>窗口中按下<code>ctrl+a</code>然后再按下<code>c</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen</span><br></pre></td></tr></table></figure>
<p><img src="/images/tmux-vs-screen/simple-screen.png" alt="效果图" /></p>
<h3 id="创建终端并运行命令"><a class="markdownIt-Anchor" href="#创建终端并运行命令"></a> 创建终端并运行命令</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen vi main.c</span><br></pre></td></tr></table></figure>
<p><img src="/images/tmux-vs-screen/screen-simple-command.png" alt="效果图" /></p>
<h3 id="离开screen终端"><a class="markdownIt-Anchor" href="#离开screen终端"></a> 离开screen终端</h3>
<p>在<code>screen</code>窗口中按下<code>ctrl+a</code>然后再按下<code>d</code></p>
<h3 id="显示已经创建的screen"><a class="markdownIt-Anchor" href="#显示已经创建的screen"></a> 显示已经创建的screen</h3>
<p>在<code>screen</code>窗口中按下<code>ctrl+a</code>然后再按下<code>&quot;</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen -ls</span><br></pre></td></tr></table></figure>
<p><img src="/images/tmux-vs-screen/screen-list.png" alt="效果图" /><br />
<img src="/images/tmux-vs-screen/screen-list2.png" alt="效果图" /></p>
<h3 id="重新连接会话"><a class="markdownIt-Anchor" href="#重新连接会话"></a> 重新连接会话</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">screen -r &lt;id&gt;</span><br></pre></td></tr></table></figure>
<h3 id="水平分割"><a class="markdownIt-Anchor" href="#水平分割"></a> 水平分割</h3>
<p>在<code>screen</code>窗口中按下<code>ctrl+a</code>然后再按下<code>|</code>，只会分割而不会创建新的session。使用<code>ctrl+a</code>然后按下<code>tab</code>切换窗口，<code>ctrl+a</code>然后按下<code>c</code>新建session。下同<br />
<img src="/images/tmux-vs-screen/screen-horizontally.png" alt="效果图" /></p>
<h3 id="screen竖直分割"><a class="markdownIt-Anchor" href="#screen竖直分割"></a> screen竖直分割</h3>
<p>在<code>screen</code>窗口中按下<code>ctrl+a</code>然后再按下<code>S</code>。注意大写<br />
<img src="/images/tmux-vs-screen/screen-vertically.png" alt="效果图" /></p>
<h3 id="分割中切换窗口"><a class="markdownIt-Anchor" href="#分割中切换窗口"></a> 分割中切换窗口</h3>
<p>使用<code>ctrl+a</code>然后按下<code>tab</code>切换窗口</p>
<h3 id="杀死session"><a class="markdownIt-Anchor" href="#杀死session"></a> 杀死session</h3>
<p>使用<code>ctrl+d</code></p>
<h1 id="对比"><a class="markdownIt-Anchor" href="#对比"></a> 对比</h1>
<p><code>screen</code>和<code>tmux</code>都是终端复用，大体的功能上都差不多，<code>tmux</code>是BSD协议，<code>screen</code>是GNU协议。从个人感觉上<code>tmux</code>对个人更友好，在分割窗口 时候自动创建新会话，同时有状态条显示，还可以自动命名窗口，这是<code>screen</code>没有的。<code>screen</code>可以和其他用户分享会话，而<code>tmux</code>不行。</p>
<h1 id="wsl中提示screen需要root权限解决"><a class="markdownIt-Anchor" href="#wsl中提示screen需要root权限解决"></a> wsl中提示screen需要root权限解决</h1>
<p>参考<a href="https://superuser.com/questions/1195962/cannot-make-directory-var-run-screen-permission-denied">superuser</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir ~/.screen &amp;&amp; chmod 700 ~/.screen</span><br></pre></td></tr></table></figure>
<p>可以把下面这一句放进<code>~/.bashrc</code></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> SCREENDIR=<span class="variable">$HOME</span>/.screen</span><br></pre></td></tr></table></figure>
<h1 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h1>
<p><a href="https://linuxhint.com/tmux_vs_screen/">Tmux vs. Screen tool comparison</a><br />
<a href="https://www.howtogeek.com/671422/how-to-use-tmux-on-linux-and-why-its-better-than-screen/">How to Use tmux on Linux (and Why It’s Better Than Screen)</a><br />
<a href="https://blog.csdn.net/yuanxinfei920/article/details/78712990">使用tmux分屏（既可以左右分屏，也可以上下分屏）</a><br />
<a href="http://www.ruanyifeng.com/blog/2019/10/tmux.html">Tmux 使用教程</a><br />
<a href="https://www.runoob.com/linux/linux-comm-screen.html">Linux screen命令</a><br />
<a href="https://superuser.com/questions/1195962/cannot-make-directory-var-run-screen-permission-denied">Cannot make directory ‘/var/run/screen’: Permission denied</a></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Network in Network 学习笔记</title>
    <url>/2021/03/29/NIN-Note/</url>
    <content><![CDATA[<p>因为在YOLO里面看到了使用Global Average Pooling替换全连接层，同时也在其他网络里面发现也都使用了Global Average Pooling替换全连接层。因此，打算来读一下这篇提出了Global Average Pooling的Network in Network。</p>
<p>封面来源《9nine》</p>
<span id="more"></span>
<h1 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h1>
<p>  <a href="https://arxiv.org/abs/1312.4400">Network in Network</a>（下面简称NIN）认为CNN中的卷积核是基础数据补丁的广义线性模型（GLM），同时他认为GLM的抽象层次比较低。用更有效的非线性函数逼近器代替GLM，可以提高局部模型的抽象能力。当潜在概念的样本线性可分离时，GLM可以达到很好的抽象程度，即概念的变体都存在于GLM定义的分离平面的一侧。因此，传统的CNN隐式地假设潜在概念是线性可分的。然而，同一概念的数据往往存在于非线性流形上，因此捕获这些概念的表示通常是输入的高度非线性的函数。在NIN中使用了一种通用的非线性函数逼近器的“微型网络”结果来替代GLM。在NIN一文中就是选择了多层感知器MLP作为非线性函数逼近器。<br />
  在NIN一文中提出了两个创新点：Mlpconv和Global Average Pooling</p>
<h1 id="mlpconv"><a class="markdownIt-Anchor" href="#mlpconv"></a> Mlpconv</h1>
<p>  在没有关于潜在先验分布的情况下，使用通用函数逼近器来提取局部补丁的特征是可行的，因为它能够逼近潜在概念的更抽象的表示。径向基网络（Radial basis network）和多层感知器（multilayer perceptron）是两个知名的通用函数逼近器。NIN一文中选择使用多层感知是基于以下两个原因。这种新型的层就叫做mlpconv。</p>
<ol>
<li>多层感知器和卷积神经网络一样，都是通过反向传播进行训练</li>
<li>多层感知器自身也是一个深度模型，符合特征再利用的原则</li>
</ol>
<h2 id="传统卷积"><a class="markdownIt-Anchor" href="#传统卷积"></a> 传统卷积</h2>
<p><img src="/images/NIN-Note/linear-conv.png" alt="" /><br />
GLM中的卷积结构如上图所示，以激活函数为Relu为例子，其计算如下</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msubsup><mi>ω</mi><mi>k</mi><mi>T</mi></msubsup><msub><mi>x</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{i,j,k} = max(\omega_k^Tx_{i,j},0)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.177439em;vertical-align:-0.286108em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913309999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span></p>
<h2 id="mlp卷积"><a class="markdownIt-Anchor" href="#mlp卷积"></a> mlp卷积</h2>
<p><img src="/images/NIN-Note/mlpconv-layer.png" alt="" /><br />
mlp中的卷积格式如上所示，以Relu激活函数为例子，其计算公式如下</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>f</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><msub><mi>k</mi><mn>1</mn></msub></mrow><mn>1</mn></msubsup><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msup><msubsup><mi>ω</mi><msub><mi>k</mi><mn>1</mn></msub><mn>1</mn></msubsup><mi>T</mi></msup><msub><mi>x</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><msub><mi>k</mi><mn>1</mn></msub></msub><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">⋮</mi><mpadded height="+0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msubsup><mi>f</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo separator="true">,</mo><msub><mi>k</mi><mi>n</mi></msub></mrow><mi>n</mi></msubsup><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msup><msubsup><mi>ω</mi><msub><mi>k</mi><mi>n</mi></msub><mi>n</mi></msubsup><mi>T</mi></msup><msubsup><mi>f</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>+</mo><msub><mi>b</mi><msub><mi>k</mi><mi>n</mi></msub></msub><mo separator="true">,</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{matrix}
f_{i,j,k_1}^1 = max({\omega_{k_1}^1}^Tx_{i,j}+b_{k_1},0)\\ 
\vdots \\
f_{i,j,k_n}^n = max({\omega_{k_n}^n}^Tf_{i,j}^{n-1}+b_{k_n},0)
\end{matrix}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:4.639393999999999em;vertical-align:-2.0696969999999997em;"></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.5696969999999997em;"><span style="top:-5.211857999999999em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4168920000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03148em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192159999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4168920000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03148em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3832079999999999em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0453389999999998em;"><span style="top:-3.267008em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31731428571428577em;"><span style="top:-2.357em;margin-left:-0.03148em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span><span style="top:-3.2926419999999994em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width:0em;border-top-width:1.5em;bottom:0em;"></span></span></span></span><span style="top:-2.0370190000000004em;"><span class="pstrut" style="height:3.6875em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4168920000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03148em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192159999999999em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">ω</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-2.4168920000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03148em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3832079999999999em;"><span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.895623em;"><span style="top:-3.117292em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.854239em;"><span style="top:-2.4231360000000004em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1031310000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.412972em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285719em;"><span style="top:-2.357em;margin-left:-0.03148em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">0</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0696969999999997em;"><span></span></span></span></span></span></span></span></span></span></span></span></p>
<p>  这种跨通道参数池化层与1×1的卷积核等效，实际上就是1×1的卷积实现。这使得NIN的结构容易理解。</p>
<h2 id="关于11卷积"><a class="markdownIt-Anchor" href="#关于11卷积"></a> 关于1×1卷积</h2>
<p><img src="/images/NIN-Note/conv.gif" alt="conv" /><br />
  卷积核的计算如上图所示，输出通道与卷积核的个数是相等的。对于输入单通道的单个1×1卷积核，此时就相当于feature map乘以一个系数，而当输入为多个通道的时候，相当于把多个feature map线性加权求和再通过一个激活函数，其功能与全连接相似，权重共享，参数更少。</p>
<h1 id="global-average-pooling"><a class="markdownIt-Anchor" href="#global-average-pooling"></a> Global Average Pooling</h1>
<p>  传统的卷积神经网络在做分类任务的时候，会对最后一个卷积层得到的特征图向量化并送入全连接层，再经过一个softmax逻辑回归。但是全连接层容易过拟合，阻碍网络泛化性，直到Hinton等人提出dropout，提升了其泛化能力，预防过拟合。<br />
  NIN中提出了另一种策略，也就是全局平均池化来替代全连接。在最后一个mlpconv层生成的每张特征图直接求平均，得到结果向量输入到softmax。全局平均池化相比全连接层的优点在于通过增强特征图与类比间的对应关系使卷积结构保留的更好，使特征图分类是可信的得到很好的解释；另一个优点是全局平均池化层中没有参数需要优化，因此避免了过拟合；此外，全局平均池化汇聚了空间信息，所以对输入的空间转换更有鲁棒性。</p>
<h1 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h1>
<ol>
<li>改变卷积结构，使用1×1卷积，相当于一个微型的全连接层，提高卷积层抽象能力。</li>
<li>使用全局平均池化替代全连接避免过拟合，对输入的空间转换更有鲁棒性。</li>
</ol>
<h1 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h1>
<p>[1] Lin M ,  Chen Q ,  Yan S . Network In Network[J]. Computer Science, 2013.</p>
]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>毕业设计（二）darknet编译</title>
    <url>/2021/03/04/builddarknet/</url>
    <content><![CDATA[<h2 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h2>
<p>寒假训练了4个版本的yolo，其中yolov4和yolov4-tiny是基于darknet的。由于都是在<a href="https://colab.research.google.com/notebooks/welcome.ipynb">colab</a>上训练的，不好展示，所以打算在win10上跑一下，因此本文旨在win10编译darknet的踩坑记录。</p>
<h2 id="安装visual-studio"><a class="markdownIt-Anchor" href="#安装visual-studio"></a> 安装Visual Studio</h2>
<p>这个不用多说，安装2017或者2019版就行，需要开启英语语言</p>
<h2 id="安装cuda"><a class="markdownIt-Anchor" href="#安装cuda"></a> 安装CUDA</h2>
<p>参考上一篇安装CUDA就行，需要CUDAv10.0以上，并且安装时要点VS集成</p>
<h2 id="vcpkg安装"><a class="markdownIt-Anchor" href="#vcpkg安装"></a> vcpkg安装</h2>
<p>参考vcpkg的官方下载方式，<code>powershell</code>中执行如下命令</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/microsoft/vcpkg</span><br><span class="line"><span class="built_in">cd</span> vcpkg</span><br><span class="line">bootstrap<span class="literal">-vcpkg</span>.bat</span><br></pre></td></tr></table></figure>
<p>如果执行<code>bootstrap-vcpkg.bat</code>时，下载<code>vcpkg.exe</code>遇到网络问题，可以直接去<code>release</code>里面下载二进制文件</p>
<h2 id="vcpkg安装依赖"><a class="markdownIt-Anchor" href="#vcpkg安装依赖"></a> vcpkg安装依赖</h2>
<p>根据AlexeyAB的repo说明，<code>powershell</code>执行如下命令</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">vcpkg install darknet[<span class="type">full</span>]:x64<span class="literal">-windows</span></span><br></pre></td></tr></table></figure>
<h3 id="遇到的问题"><a class="markdownIt-Anchor" href="#遇到的问题"></a> 遇到的问题</h3>
<p><strong>事后得知，此处遇到的网络问题可以试一试校园网，校园网似乎对从外网下载文件有较好的效果</strong></p>
<p>安装opencv时，需要从<code>raw.githubusercontent.com</code>下载东西，然而遇到了host问题。首先尝试修改了host，结果遇到了<code>SSL connect error</code>问题，于是在加了几个host发现还是失败了。遂尝试使用VPN代理，按照输出将<code>HTTP_PROXY</code>和<code>HTTPS_PROXY</code>加入到环境变量，依然不行。尝试直接在powershell中使用变量。</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="variable">$env:HTTPS_PROXY</span>=<span class="string">&quot;https://127.0.0.1:7890/&quot;</span></span><br><span class="line"><span class="variable">$env:HTTP_PROXY</span>=<span class="string">&quot;http://127.0.0.1:7890/&quot;</span></span><br></pre></td></tr></table></figure>
<p>此时的staus输出为</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">Failed. Status: <span class="number">4</span>;<span class="string">&quot;A requested feature, protocol or option was not found built-in in this libcurl due to a build-time decision.&quot;</span></span><br><span class="line"></span><br><span class="line">Failed to download file.</span><br><span class="line"><span class="keyword">If</span> you use a proxy, please <span class="built_in">set</span> the HTTPS_PROXY and HTTP_PROXY environment variables to <span class="string">&quot;https://user:password@your-proxy-ip-address:port/&quot;</span>.</span><br><span class="line"></span><br><span class="line"><span class="keyword">If</span> error with status <span class="number">4</span> (Issue <span class="comment">#15434),</span></span><br><span class="line"><span class="keyword">try</span> setting <span class="string">&quot;http://user:password@your-proxy-ip-address:port/&quot;</span>.</span><br><span class="line"></span><br><span class="line">Otherwise, please submit an issue at https://github.com/Microsoft/vcpkg/issues</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>因此根据要求执行下面代码</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="variable">$env:HTTPS_PROXY</span>=<span class="string">&quot;http://127.0.0.1:7890/&quot;</span></span><br></pre></td></tr></table></figure>
<p>此时重新执行安装命令，即可完成下载。</p>
<p>安装opencv时遇到</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Error at scripts&#x2F;cmake&#x2F;vcpkg_execute_build_process.cmake:144 (message):</span><br><span class="line">    Command failed: C:&#x2F;src&#x2F;vcpkg&#x2F;downloads&#x2F;tools&#x2F;cmake-3.19.2-windows&#x2F;cmake-3.19.2-win32-x86&#x2F;bin&#x2F;cmake.exe --build . --config Debug --target install -- -v -j9</span><br><span class="line">    Working Directory: C:&#x2F;src&#x2F;vcpkg&#x2F;buildtrees&#x2F;opencv4&#x2F;x64-windows-dbg</span><br><span class="line">    See logs for more information:</span><br><span class="line">      C:\src\vcpkg\buildtrees\opencv4\install-x64-windows-dbg-out.log</span><br><span class="line"></span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  scripts&#x2F;cmake&#x2F;vcpkg_build_cmake.cmake:96 (vcpkg_execute_build_process)</span><br><span class="line">  scripts&#x2F;cmake&#x2F;vcpkg_install_cmake.cmake:27 (vcpkg_build_cmake)</span><br><span class="line">  ports&#x2F;opencv4&#x2F;portfile.cmake:385 (vcpkg_install_cmake)</span><br><span class="line">  scripts&#x2F;ports.cmake:133 (include)</span><br></pre></td></tr></table></figure>
<p>尝试如下指令</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">vcpkg install darknet[<span class="type">opencv</span>-<span class="type">base</span>,<span class="type">cuda</span>,<span class="type">cudnn</span>]:x64<span class="literal">-windows</span></span><br></pre></td></tr></table></figure>
<h2 id="编译darknet"><a class="markdownIt-Anchor" href="#编译darknet"></a> 编译darknet</h2>
<p>提前安装<a href="https://github.com/ninja-build/ninja">ninja</a>并添加到环境变量</p>
<p>修改<code>Makefile</code></p>
<figure class="highlight makefile"><table><tr><td class="code"><pre><span class="line">GPU=1</span><br><span class="line">CUDNN=1</span><br><span class="line">CUDNN_HALF=1</span><br><span class="line">OPENCV=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#我是gtx1050，根据makefile里面的注释自己的修改</span></span><br><span class="line">ARCH= -gencode arch=compute_61,code=sm_61 -gencode arch=compute_61,code=compute_61 </span><br></pre></td></tr></table></figure>
<p>按照AlexeyAB的repo，执行如下代码，注意CMake的版本，过低将无法编译。注意下面的顺序，如果有问题需要清理一下缓存，不然会一直编译失败</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="variable">$env:VCPKG_ROOT</span>=<span class="string">&quot;vcpkg的位置&quot;</span></span><br><span class="line">git clone https://github.com/AlexeyAB/darknet</span><br><span class="line"><span class="built_in">cd</span> darknet</span><br><span class="line">powershell <span class="literal">-ExecutionPolicy</span> Bypass <span class="operator">-File</span> .\build.ps1</span><br></pre></td></tr></table></figure>
<p>如果正确执行上述步骤的话，编译是不会出错的</p>
<h2 id="测试darknet"><a class="markdownIt-Anchor" href="#测试darknet"></a> 测试darknet</h2>
<h3 id="原图"><a class="markdownIt-Anchor" href="#原图"></a> 原图</h3>
<p><img src="/images/GraduationProject/origin.jpg" alt="origin" /></p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">darknet.exe detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights origin.jpg  <span class="literal">-thresh</span> <span class="number">0.25</span></span><br></pre></td></tr></table></figure>
<h3 id="输出"><a class="markdownIt-Anchor" href="#输出"></a> 输出</h3>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CUDA-version: 11000 (11000), cuDNN: 8.0.4, GPU count: 1  </span><br><span class="line"> OpenCV version: 4.5.1</span><br><span class="line"> 0 : compute_capability &#x3D; 610, cudnn_half &#x3D; 0, GPU: GeForce GTX 1050 </span><br><span class="line">net.optimized_memory &#x3D; 0 </span><br><span class="line">mini_batch &#x3D; 1, batch &#x3D; 8, time_steps &#x3D; 1, train &#x3D; 0</span><br><span class="line">   layer   filters  size&#x2F;strd(dil)      input                output</span><br><span class="line">   0 Create CUDA-stream - 0</span><br><span class="line"> Create cudnn-handle 0 </span><br><span class="line">conv     32       3 x 3&#x2F; 1    608 x 608 x   3 -&gt;  608 x 608 x  32 0.639 BF</span><br><span class="line">   1 conv     64       3 x 3&#x2F; 2    608 x 608 x  32 -&gt;  304 x 304 x  64 3.407 BF</span><br><span class="line">   2 conv     64       1 x 1&#x2F; 1    304 x 304 x  64 -&gt;  304 x 304 x  64 0.757 BF</span><br><span class="line">   3 route  1                                      -&gt;  304 x 304 x  64 </span><br><span class="line">   4 conv     64       1 x 1&#x2F; 1    304 x 304 x  64 -&gt;  304 x 304 x  64 0.757 BF</span><br><span class="line">   5 conv     32       1 x 1&#x2F; 1    304 x 304 x  64 -&gt;  304 x 304 x  32 0.379 BF</span><br><span class="line">   6 conv     64       3 x 3&#x2F; 1    304 x 304 x  32 -&gt;  304 x 304 x  64 3.407 BF</span><br><span class="line">   7 Shortcut Layer: 4,  wt &#x3D; 0, wn &#x3D; 0, outputs: 304 x 304 x  64 0.006 BF</span><br><span class="line">   8 conv     64       1 x 1&#x2F; 1    304 x 304 x  64 -&gt;  304 x 304 x  64 0.757 BF</span><br><span class="line">   9 route  8 2                                    -&gt;  304 x 304 x 128 </span><br><span class="line">  10 conv     64       1 x 1&#x2F; 1    304 x 304 x 128 -&gt;  304 x 304 x  64 1.514 BF</span><br><span class="line">  11 conv    128       3 x 3&#x2F; 2    304 x 304 x  64 -&gt;  152 x 152 x 128 3.407 BF</span><br><span class="line">  12 conv     64       1 x 1&#x2F; 1    152 x 152 x 128 -&gt;  152 x 152 x  64 0.379 BF</span><br><span class="line">  13 route  11                                     -&gt;  152 x 152 x 128</span><br><span class="line">  14 conv     64       1 x 1&#x2F; 1    152 x 152 x 128 -&gt;  152 x 152 x  64 0.379 BF</span><br><span class="line">  15 conv     64       1 x 1&#x2F; 1    152 x 152 x  64 -&gt;  152 x 152 x  64 0.189 BF</span><br><span class="line">  16 conv     64       3 x 3&#x2F; 1    152 x 152 x  64 -&gt;  152 x 152 x  64 1.703 BF</span><br><span class="line">  17 Shortcut Layer: 14,  wt &#x3D; 0, wn &#x3D; 0, outputs: 152 x 152 x  64 0.001 BF</span><br><span class="line">  18 conv     64       1 x 1&#x2F; 1    152 x 152 x  64 -&gt;  152 x 152 x  64 0.189 BF</span><br><span class="line">  19 conv     64       3 x 3&#x2F; 1    152 x 152 x  64 -&gt;  152 x 152 x  64 1.703 BF</span><br><span class="line">  20 Shortcut Layer: 17,  wt &#x3D; 0, wn &#x3D; 0, outputs: 152 x 152 x  64 0.001 BF</span><br><span class="line">  21 conv     64       1 x 1&#x2F; 1    152 x 152 x  64 -&gt;  152 x 152 x  64 0.189 BF</span><br><span class="line">  22 route  21 12                                  -&gt;  152 x 152 x 128 </span><br><span class="line">  23 conv    128       1 x 1&#x2F; 1    152 x 152 x 128 -&gt;  152 x 152 x 128 0.757 BF</span><br><span class="line">  24 conv    256       3 x 3&#x2F; 2    152 x 152 x 128 -&gt;   76 x  76 x 256 3.407 BF</span><br><span class="line">  25 conv    128       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 128 0.379 BF</span><br><span class="line">  26 route  24                                     -&gt;   76 x  76 x 256</span><br><span class="line">  27 conv    128       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 128 0.379 BF</span><br><span class="line">  28 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  29 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  30 Shortcut Layer: 27,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  31 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  32 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  33 Shortcut Layer: 30,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  34 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  35 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  36 Shortcut Layer: 33,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  37 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  38 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  39 Shortcut Layer: 36,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  40 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  41 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  42 Shortcut Layer: 39,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  43 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  44 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  45 Shortcut Layer: 42,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  46 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  47 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  48 Shortcut Layer: 45,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  49 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  50 conv    128       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 1.703 BF</span><br><span class="line">  51 Shortcut Layer: 48,  wt &#x3D; 0, wn &#x3D; 0, outputs:  76 x  76 x 128 0.001 BF</span><br><span class="line">  52 conv    128       1 x 1&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 128 0.189 BF</span><br><span class="line">  53 route  52 25                                  -&gt;   76 x  76 x 256 </span><br><span class="line">  54 conv    256       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 256 0.757 BF</span><br><span class="line">  55 conv    512       3 x 3&#x2F; 2     76 x  76 x 256 -&gt;   38 x  38 x 512 3.407 BF</span><br><span class="line">  56 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line">  57 route  55                                     -&gt;   38 x  38 x 512</span><br><span class="line">  58 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line">  59 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  60 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  61 Shortcut Layer: 58,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  62 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  63 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  64 Shortcut Layer: 61,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  65 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  66 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  67 Shortcut Layer: 64,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  68 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  69 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  70 Shortcut Layer: 67,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  71 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  72 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  73 Shortcut Layer: 70,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  74 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  75 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  76 Shortcut Layer: 73,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  77 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  78 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  79 Shortcut Layer: 76,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  80 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  81 conv    256       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 1.703 BF</span><br><span class="line">  82 Shortcut Layer: 79,  wt &#x3D; 0, wn &#x3D; 0, outputs:  38 x  38 x 256 0.000 BF</span><br><span class="line">  83 conv    256       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 256 0.189 BF</span><br><span class="line">  84 route  83 56                                  -&gt;   38 x  38 x 512 </span><br><span class="line">  85 conv    512       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 512 0.757 BF</span><br><span class="line">  86 conv   1024       3 x 3&#x2F; 2     38 x  38 x 512 -&gt;   19 x  19 x1024 3.407 BF</span><br><span class="line">  87 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line">  88 route  86                                     -&gt;   19 x  19 x1024</span><br><span class="line">  89 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line">  90 conv    512       1 x 1&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.189 BF</span><br><span class="line">  91 conv    512       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 1.703 BF</span><br><span class="line">  92 Shortcut Layer: 89,  wt &#x3D; 0, wn &#x3D; 0, outputs:  19 x  19 x 512 0.000 BF</span><br><span class="line">  93 conv    512       1 x 1&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.189 BF</span><br><span class="line">  94 conv    512       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 1.703 BF</span><br><span class="line">  95 Shortcut Layer: 92,  wt &#x3D; 0, wn &#x3D; 0, outputs:  19 x  19 x 512 0.000 BF</span><br><span class="line">  96 conv    512       1 x 1&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.189 BF</span><br><span class="line">  97 conv    512       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 1.703 BF</span><br><span class="line">  98 Shortcut Layer: 95,  wt &#x3D; 0, wn &#x3D; 0, outputs:  19 x  19 x 512 0.000 BF</span><br><span class="line">  99 conv    512       1 x 1&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.189 BF</span><br><span class="line"> 100 conv    512       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 1.703 BF</span><br><span class="line"> 101 Shortcut Layer: 98,  wt &#x3D; 0, wn &#x3D; 0, outputs:  19 x  19 x 512 0.000 BF</span><br><span class="line"> 102 conv    512       1 x 1&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.189 BF</span><br><span class="line"> 103 route  102 87                                 -&gt;   19 x  19 x1024</span><br><span class="line"> 104 conv   1024       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x1024 0.757 BF</span><br><span class="line"> 105 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line"> 106 conv   1024       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x1024 3.407 BF</span><br><span class="line"> 107 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line"> 108 max                5x 5&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.005 BF</span><br><span class="line"> 109 route  107                                            -&gt;   19 x  19 x 512</span><br><span class="line"> 110 max                9x 9&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.015 BF</span><br><span class="line"> 111 route  107                                            -&gt;   19 x  19 x 512</span><br><span class="line"> 112 max               13x13&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 512 0.031 BF</span><br><span class="line"> 113 route  112 110 108 107                        -&gt;   19 x  19 x2048</span><br><span class="line"> 114 conv    512       1 x 1&#x2F; 1     19 x  19 x2048 -&gt;   19 x  19 x 512 0.757 BF</span><br><span class="line"> 115 conv   1024       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x1024 3.407 BF</span><br><span class="line"> 116 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line"> 117 conv    256       1 x 1&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x 256 0.095 BF</span><br><span class="line"> 118 upsample                 2x    19 x  19 x 256 -&gt;   38 x  38 x 256</span><br><span class="line"> 119 route  85                                     -&gt;   38 x  38 x 512</span><br><span class="line"> 120 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 121 route  120 118                                -&gt;   38 x  38 x 512 </span><br><span class="line"> 122 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 123 conv    512       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 512 3.407 BF</span><br><span class="line"> 124 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 125 conv    512       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 512 3.407 BF</span><br><span class="line"> 126 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 127 conv    128       1 x 1&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 128 0.095 BF</span><br><span class="line"> 128 upsample                 2x    38 x  38 x 128 -&gt;   76 x  76 x 128</span><br><span class="line"> 129 route  54                                     -&gt;   76 x  76 x 256</span><br><span class="line"> 130 conv    128       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 128 0.379 BF</span><br><span class="line"> 131 route  130 128                                -&gt;   76 x  76 x 256</span><br><span class="line"> 132 conv    128       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 128 0.379 BF</span><br><span class="line"> 133 conv    256       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 256 3.407 BF</span><br><span class="line"> 134 conv    128       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 128 0.379 BF</span><br><span class="line"> 135 conv    256       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 256 3.407 BF</span><br><span class="line"> 136 conv    128       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 128 0.379 BF</span><br><span class="line"> 137 conv    256       3 x 3&#x2F; 1     76 x  76 x 128 -&gt;   76 x  76 x 256 3.407 BF</span><br><span class="line"> 138 conv    255       1 x 1&#x2F; 1     76 x  76 x 256 -&gt;   76 x  76 x 255 0.754 BF</span><br><span class="line"> 139 yolo</span><br><span class="line">[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.20</span><br><span class="line">nms_kind: greedynms (1), beta &#x3D; 0.600000</span><br><span class="line"> 140 route  136                                            -&gt;   76 x  76 x 128</span><br><span class="line"> 141 conv    256       3 x 3&#x2F; 2     76 x  76 x 128 -&gt;   38 x  38 x 256 0.852 BF</span><br><span class="line"> 142 route  141 126                                -&gt;   38 x  38 x 512 </span><br><span class="line"> 143 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 144 conv    512       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 512 3.407 BF</span><br><span class="line"> 145 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 146 conv    512       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 512 3.407 BF</span><br><span class="line"> 147 conv    256       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 256 0.379 BF</span><br><span class="line"> 148 conv    512       3 x 3&#x2F; 1     38 x  38 x 256 -&gt;   38 x  38 x 512 3.407 BF</span><br><span class="line"> 149 conv    255       1 x 1&#x2F; 1     38 x  38 x 512 -&gt;   38 x  38 x 255 0.377 BF</span><br><span class="line"> 150 yolo</span><br><span class="line">[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.10</span><br><span class="line">nms_kind: greedynms (1), beta &#x3D; 0.600000</span><br><span class="line"> 151 route  147                                            -&gt;   38 x  38 x 256</span><br><span class="line"> 152 conv    512       3 x 3&#x2F; 2     38 x  38 x 256 -&gt;   19 x  19 x 512 0.852 BF</span><br><span class="line"> 153 route  152 116                                -&gt;   19 x  19 x1024</span><br><span class="line"> 154 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line"> 155 conv   1024       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x1024 3.407 BF</span><br><span class="line"> 156 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line"> 157 conv   1024       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x1024 3.407 BF</span><br><span class="line"> 158 conv    512       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 512 0.379 BF</span><br><span class="line"> 159 conv   1024       3 x 3&#x2F; 1     19 x  19 x 512 -&gt;   19 x  19 x1024 3.407 BF</span><br><span class="line"> 160 conv    255       1 x 1&#x2F; 1     19 x  19 x1024 -&gt;   19 x  19 x 255 0.189 BF</span><br><span class="line"> 161 yolo</span><br><span class="line">[yolo] params: iou loss: ciou (4), iou_norm: 0.07, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.05</span><br><span class="line">nms_kind: greedynms (1), beta &#x3D; 0.600000</span><br><span class="line">Total BFLOPS 128.459</span><br><span class="line">avg_outputs &#x3D; 1068395</span><br><span class="line"> Allocate additional workspace_size &#x3D; 52.43 MB</span><br><span class="line">Loading weights from yolov4.weights...</span><br><span class="line"> seen 64, trained: 32032 K-images (500 Kilo-batches_64)</span><br><span class="line">Done! Loaded 162 layers from weights-file </span><br><span class="line"> Detection layer: 139 - type &#x3D; 28 </span><br><span class="line"> Detection layer: 150 - type &#x3D; 28 </span><br><span class="line"> Detection layer: 161 - type &#x3D; 28</span><br><span class="line">origin.jpg: Predicted in 184.923000 milli-seconds.</span><br><span class="line">person: 98%</span><br><span class="line">car: 94%</span><br><span class="line">truck: 31%</span><br><span class="line">car: 88%</span><br><span class="line">car: 99%</span><br><span class="line">person: 99%</span><br><span class="line">person: 37%</span><br><span class="line">car: 99%</span><br><span class="line">bus: 57%</span><br><span class="line">truck: 60%</span><br><span class="line">car: 99%</span><br></pre></td></tr></table></figure>
<h3 id="结果"><a class="markdownIt-Anchor" href="#结果"></a> 结果</h3>
<p>可以看到预测结果，同时darknet里面也输出了GPU的信息<br />
<img src="/images/GraduationProject/predictions.jpg" alt="predictions" /></p>
<h2 id="结论"><a class="markdownIt-Anchor" href="#结论"></a> 结论</h2>
<p>win10版本的darknet gpu版本安装完成。道路坎坷，记录下踩过的坑</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>毕业设计</tag>
        <tag>yolo</tag>
        <tag>darknet</tag>
      </tags>
  </entry>
</search>
